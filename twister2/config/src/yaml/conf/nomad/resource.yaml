# working directory
twister2.resource.scheduler.mpi.working.directory: "${HOME}/.twister2/jobs"

twister2.resource.job.package.url: "http://149.165.xxx.xx:8082/twister2/mesos/twister2-job.tar.gz"

twister2.resource.core.package.url: "http://149.165.xxx.xx:8082/twister2/mesos/twister2-core-0.8.0.tar.gz"


# the launcher class
twister2.resource.class.launcher: "edu.iu.dsc.tws.rsched.schedulers.nomad.NomadLauncher"

#The URI of Nomad
#twister2.resource.nomad.scheduler.uri: "http://149.165.xxx.xx:4646"
twister2.resource.nomad.scheduler.uri: "http://localhost:4646"



# The nomad schedules cpu resources in terms of clock frequency (e.g. MHz), while Heron topologies
# specify cpu requests in term of cores.  This config maps core to clock freqency.
twister2.resource.nomad.core.freq.mapping: 2000


# weather we are in a shared file system, if that is the case, each worker will not download the
# core package and job package, otherwise they will download those packages
twister2.resource.filesystem.shared: true

# name of the script
twister2.resource.nomad.shell.script: "nomad.sh"

# path to the system core package
twister2.resource.system.package.uri: "${TWISTER2_DIST}/twister2-core-0.8.0.tar.gz"

# the directory where the file will be uploaded, make sure the user has the necessary permissions
# to upload the file here.

#twister2.uploader.directory: "/root/.twister2/repository"
# if you want to run it on a local machine use this value
twister2.resource.uploader.directory: "/root/.twister2/repository/"

# if you want to use http server on echo
#twister2.resource.uploader.directory: "/var/www/html/twister2/mesos/"


# This is the scp command options that will be used by the uploader, this can be used to
# specify custom options such as the location of ssh keys.
twister2.resource.uploader.scp.command.options: "--chmod=+rwx"

# The scp connection string sets the remote user name and host used by the uploader.
#twister2.resource.uploader.scp.command.connection: "root@149.165.xxx.xx"
twister2.resource.uploader.scp.command.connection: "root@localhost"

# The ssh command options that will be used when connecting to the uploading host to execute
# command such as delete files, make directories.
twister2.resource.uploader.ssh.command.options: ""

# The ssh connection string sets the remote user name and host used by the uploader.
#twister2.resource.uploader.ssh.command.connection: "root@149.165.xxx.xx"
twister2.resource.uploader.ssh.command.connection: "root@localhost"

# file system uploader to be used
twister2.resource.class.uploader: "edu.iu.dsc.tws.rsched.uploaders.localfs.LocalFileSystemUploader"
# twister2.resource.class.uploader: "edu.iu.dsc.tws.rsched.uploaders.scp.ScpUploader"

# this is the method that workers use to download the core and job packages
# it could be  LOCAL,  HTTP, HDFS, ..
#twister2.resource.uploader.download.method: "HTTP"
twister2.resource.uploader.download.method: "LOCAL"

########################################################################################
# client related configurations for job submit
########################################################################################

# nfs server address
#twister2.resource.nfs.server.address: "149.165.xxx.xx"
twister2.resource.nfs.server.address: "localhost"

# nfs server path
#twister2.resource.nfs.server.path: "/nfs/shared/twister2"
twister2.resource.nfs.server.path: "/tmp/logs"

# rack label key for Mesos nodes in a cluster
# each rack should have a unique label
# all nodes in a rack should share this label
# Twister2 workers can be scheduled by using these label values
# Better data locality can be achieved
# Example: rack=rack1, rack=rack2, rack=rack3, etc
# no default value is specified
twister2.resource.rack.labey.key: rack

# data center label key
# each data center should have a unique label
# all nodes in a data center should share this label
# Twister2 workers can be scheduled by using these label values
# Better data locality can be achieved
# Example: datacenter=dc1, datacenter=dc1, datacenter=dc1, etc
# no default value is specified
twister2.resource.datacenter.labey.key: datacenter

# Data center list with rack names
twister2.resource.datacenters.list:
  - echo: ['blue-rack', 'green-rack']

# Rack list with node IPs in them
twister2.resource.racks.list:
  - blue-rack: ['10.0.0.40', '10.0.0.41', '10.0.0.42', '10.0.0.43', '10.0.0.44', ]
  - green-rack: ['node11.ip', 'node12.ip', 'node13.ip']

# A Twister2 job can have multiple sets of compute resources
# Four fields are mandatory: cpu, ram, disk and instances
# instances shows the number of compute resources to be started with this specification
# workersPerPod shows the number of workers on each pod in Kubernetes.
#    May be omitted in other clusters. default value is 1.
twister2.resource.worker.compute.resources:
  - cpu: 1  # number of cores for each worker, may be fractional such as 0.5 or 2.4
    ram: 1024 # ram for each worker as Mega bytes
    disk: 1.0 # volatile disk for each worker as Giga bytes
    instances: 6 # number of compute resource instances with this specification
  #  workersPerPod: 2 # number of workers on each pod in Kubernetes. May be omitted in other clusters.

  - cpu: 2  # number of cores for each worker, may be fractional such as 0.5 or 2.4
    ram: 1024 # ram for each worker as mega bytes
    disk: 1.0 # volatile disk for each worker as giga bytes. May be zero.
    instances: 4 # number of compute resource instances with this specification
#  workersPerPod: 2 # number of workers on each pod in Kubernetes. May be omitted in other clusters.

# by default each worker has one port
# additional ports can be requested for all workers in a job
# please provide the requested port names as a list
twister2.resource.worker.additional.ports: ["port1", "port2", "port3"]

# worker port
twister2.resource.worker_port: "31000"
