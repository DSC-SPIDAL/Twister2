# working directory for the topologies
twister2.resource.mesos.scheduler.working.directory: "~/.twister2/repository"#"${TWISTER2_DIST}/topologies/${CLUSTER}/${ROLE}/${TOPOLOGY}"

#directory of core package
twister2.resource.directory.core-package: "/root/.twister2/repository/twister2-core/"

# location of java - pick it up from shell environment
twister2.resource.directory.sandbox.java.home: "${JAVA_HOME}"

# The URI of Mesos Master
# twister2.mesos.master.uri: "149.165.150.81:5050"

# mesos framework name
twister2.resource.mesos.framework.name: "Twister2 framework"

twister2.resource.mesos.master.uri: "zk://localhost:2181/mesos"

twister2.resource.mesos.master.host: "localhost"
# The maximum time in milliseconds waiting for MesosFramework got registered with Mesos Master
twister2.resource.mesos.framework.staging.timeout.ms: 2000

# The maximum time in milliseconds waiting for Mesos Scheduler Driver to complete stop()
twister2.resource.mesos.scheduler.driver.stop.timeout.ms: 5000

# the path to load native mesos library
twister2.resource.mesos.native.library.path: "/usr/lib/mesos/0.28.1/lib/"

# the core package uri
twister2.resource.system.package.uri: "${TWISTER2_DIST}/twister2-core-0.7.0.tar.gz"


#overlay network name for docker containers
twister2.resource.mesos.overlay.network.name: "mesos-overlay"

twister2.resource.mesos.docker.image: "gurhangunduz/twister2-mesos:docker-mpi"

# the job package uri for mesos agent to fetch.
# For fetching http server must be running on mesos master
# twister2.resource.system.job.uri: "http://localhost:8082/twister2/mesos/twister2-job.tar.gz"

# launcher class for mesos submission
twister2.resource.class.launcher: "edu.iu.dsc.tws.rsched.schedulers.mesos.MesosLauncher"



# container class to run in workers
#twister2.resource.job.worker.class: "edu.iu.dsc.tws.examples.internal.rsched.BasicMesosWorker"
#twister2.resource.job.worker.class: "edu.iu.dsc.tws.examples.internal.rsched.BasicMpiJob"
twister2.resource.job.worker.class: "edu.iu.dsc.tws.examples.internal.comms.BroadcastCommunication"

# the Mesos worker class
twister2.resource.class.mesos.worker: "edu.iu.dsc.tws.rsched.schedulers.mesos.MesosWorker"

# the directory where the file will be uploaded, make sure the user has the necessary permissions
# to upload the file here.
twister2.resource.uploader.directory: "/var/www/html/twister2/mesos/"

#twister2.resource.uploader.directory.repository: "/var/www/html/twister2/mesos/"

# This is the scp command options that will be used by the uploader, this can be used to
# specify custom options such as the location of ssh keys.
twister2.resource.uploader.scp.command.options: "--chmod=+rwx"

# The scp connection string sets the remote user name and host used by the uploader.
twister2.resource.uploader.scp.command.connection: "root@149.165.150.81"

# The ssh command options that will be used when connecting to the uploading host to execute
# command such as delete files, make directories.
twister2.resource.uploader.ssh.command.options: ""

# The ssh connection string sets the remote user name and host used by the uploader.
twister2.resource.uploader.ssh.command.connection: "root@149.165.150.81"

# the uploader class
twister2.resource.class.uploader: "edu.iu.dsc.tws.rsched.uploaders.scp.ScpUploader"
# twister2.resource.class.uploader: "edu.iu.dsc.tws.rsched.uploaders.NullUploader"
# twister2.resource.class.uploader: "edu.iu.dsc.tws.rsched.uploaders.localfs.LocalFileSystemUploader"

# this is the method that workers use to download the core and job packages
# it could be  HTTP, HDFS, ..
twister2.resource.uploader.download.method: "HTTP"

# HTTP fetch uri
twister2.resource.HTTP.fetch.uri: "http://149.165.150.81:8082"


################################################################################
# Client configuration parameters for submission of twister2 jobs
################################################################################

# cluster name mesos scheduler runs in
twister2.resource.scheduler.mesos.cluster: "example"

# role in cluster
twister2.resource.scheduler.mesos.role: "www-data"

# environment name
twister2.resource.scheduler.mesos.env: "devel"

# mesos job name
twister2.resource.job.name: "basic-mesos"

# A Twister2 job can have multiple sets of compute resources
# Four fields are mandatory: cpu, ram, disk and instances
# instances shows the number of compute resources to be started with this specification
# workersPerPod shows the number of workers on each pod in Kubernetes.
#    May be omitted in other clusters. default value is 1.
twister2.resource.worker.compute.resources:
  - cpu: 1  # number of cores for each worker, may be fractional such as 0.5 or 2.4
    ram: 1024 # ram for each worker as Mega bytes
    disk: 1.0 # volatile disk for each worker as Giga bytes
    instances: 6 # number of compute resource instances with this specification
  #  workersPerPod: 2 # number of workers on each pod in Kubernetes. May be omitted in other clusters.

  - cpu: 2  # number of cores for each worker, may be fractional such as 0.5 or 2.4
    ram: 1024 # ram for each worker as mega bytes
    disk: 1.0 # volatile disk for each worker as giga bytes. May be zero.
    instances: 4 # number of compute resource instances with this specification
#  workersPerPod: 2 # number of workers on each pod in Kubernetes. May be omitted in other clusters.

# by default each worker has one port
# additional ports can be requested for all workers in a job
# please provide the requested port names as a list
twister2.resource.worker.additional.ports: ["port1", "port2", "port3"]

# driver class to run
twister2.resource.job.driver.class: "edu.iu.dsc.tws.examples.internal.rsched.DriverExample"

# nfs server address
twister2.resource.nfs.server.address: "149.165.150.81"

# nfs server path
twister2.resource.nfs.server.path: "/nfs/shared-mesos/twister2"

# worker port
twister2.resource.worker_port: "31000"

# desired nodes
#twister2.desired_nodes: "149.165.150.xx,149.165.150.xx,149.165.150.xx"
twister2.resource.desired_nodes: "all"

twister2.resource.use_docker_container: "true"

# rack label key for Mesos nodes in a cluster
# each rack should have a unique label
# all nodes in a rack should share this label
# Twister2 workers can be scheduled by using these label values
# Better data locality can be achieved
# Example: rack=rack1, rack=rack2, rack=rack3, etc
# no default value is specified
twister2.resource.rack.labey.key: rack

# data center label key
# each data center should have a unique label
# all nodes in a data center should share this label
# Twister2 workers can be scheduled by using these label values
# Better data locality can be achieved
# Example: datacenter=dc1, datacenter=dc1, datacenter=dc1, etc
# no default value is specified
twister2.resource.datacenter.labey.key: datacenter

# Data center list with rack names
twister2.resource.datacenters.list:
  - echo: ['blue-rack', 'green-rack']

# Rack list with node IPs in them
twister2.resource.racks.list:
  - blue-rack: ['10.0.0.40', '10.0.0.41', '10.0.0.42', '10.0.0.43', '10.0.0.44', ]
  - green-rack: ['node11.ip', 'node12.ip', 'node13.ip']